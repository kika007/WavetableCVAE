

hydra:
  run:
    dir: ./log/hydra/${model._target_}/${now:%Y%m%d}T${now:%H%M%S}-${trainer.max_epochs}epoch
  sweep:
    dir: ./log/hydra/${model._target_}/${now:%Y%m%d}T${now:%H%M%S}-${trainer.max_epochs}epoch

model:
  _target_: "src.models.cvae.LitCVAE"
  enc_cond_layer: [Falase, False, False, False]
  dec_cond_layer: [True, False, False, False]
  sample_points: 600
  sample_rate: 44100
  lr: 1e-5
  duplicate_num: 6
  latent_dim: 128

datamodule:
  _target_: "src.dataio.akwd_datamodule.AWKDDataModule"
  batch_size: 32

logger:
  _target_: pytorch_lightning.loggers.wandb.WandbLogger
  save_dir: null # "${paths.output_dir}"
  offline: False
  id: null # pass correct id to resume experiment!
  anonymous: null # enable anonymous logging
  project: 'WavetableVAE'
  log_model: True # upload lightning ckpts
  prefix: "" # a string to put at the beginning of metric keys
  # entity: "" # set to name of your wandb team
  group: ""
  tags: []
  job_type: ""

trainer:
  _target_: pytorch_lightning.Trainer
  max_epochs: 10000
  enable_checkpointing: True
  auto_lr_find: True
  auto_scale_batch_size: True

callbacks:
  _target_: "src.models.components.callback.MyPrintingCallback"

resume: null
# "/workspace/My-reserch-project/WavetableVAE/28tspm6i/checkpoints/epoch=1431-step=141768.ckpt"
save: null
seed: 42
debug_mode: False