hydra:
  run:
    dir: ./log/hydra/${model._target_}/${now:%Y%m%d}T${now:%H%M%S}-${trainer.max_epochs}epoch
  sweep:
    dir: ./log/hydra/${model._target_}/${now:%Y%m%d}T${now:%H%M%S}-${trainer.max_epochs}epoch

model:
  # _target_: "src.models.cvae.LitCVAE"
  _target_: "src.models.arvae.LitCVAE"

  ## 4layer
  # enc_cond_layer: [True, True, True, True] # [Falase, False, False, False]
  # dec_cond_layer: [True, True, True, True]  # [True, False, False, False]
  # enc_channels: [64, 128, 256, 512]
  # dec_channels: [256, 128, 64, 32]   # [64, 32, 16, 8]
  # enc_kernel_size: [9, 9, 9, 9]
  # dec_kernel_size: [8, 8, 8, 9]
  # enc_stride: [1, 1, 2, 2]
  # dec_stride: [2, 1, 2, 1]

  ## 5layer
  # enc_cond_layer: [True, True, True, True, True] # [Falase, False, False, False]
  # dec_cond_layer: [True, True, True, True, True]  # [True, False, False, False]
  # enc_channels: [16, 32, 64, 128, 256]
  # dec_channels: [128, 64, 32, 16, 8]   # [64, 32, 16, 8]
  # enc_kernel_size: [6, 7, 7, 7, 7]
  # dec_kernel_size: [7, 7, 7, 7, 8]
  # enc_stride: [2, 2, 1, 1, 1]
  # dec_stride: [1, 1, 1, 2, 2]

  ## 8layer
  # enc_cond_layer: [True, True, True, True, True, True, True, True]
  # dec_cond_layer: [True, True, True, True, True, True, True, True]
  # enc_channels: [4, 8, 16, 32, 64, 128, 256, 512]
  # dec_channels: [256, 128, 64, 32, 16, 8, 4, 2]
  # enc_kernel_size: [5, 5, 5, 5, 5, 6, 6, 6]
  # dec_kernel_size: [6, 6, 6, 6, 6, 6, 5, 5]
  # enc_stride: [1, 1, 1, 1, 1, 1, 2, 2]
  # dec_stride: [2, 2, 1, 1, 1, 1, 1, 1]

  ## 4layer
  enc_cond_layer: [True, True, True, True] # [Falase, False, False, False]
  dec_cond_layer: [True, True, True, True]  # [True, False, False, False]
  enc_channels: [64, 128, 256, 512]
  dec_channels: [256, 128, 64, 32]
  enc_kernel_size: [7, 7, 7, 5]
  dec_kernel_size: [6, 8, 7, 6]
  enc_stride: [3, 3, 3, 2]
  dec_stride: [2, 3, 3, 3]

  enc_cond_num: 4 # number of condition label channel
  dec_cond_num: 4
  sample_points: 600 # number of sample points
  sample_rate: 44100
  lr: 1e-4
  duplicate_num: 6
  warmup: 10001 # beta
  min_kl: 1e-4
  max_kl: 1e-1
  wave_loss_coef: null

datamodule:
  _target_: "src.dataio.akwd_datamodule.AWKDDataModule"
  batch_size: 32

logger:
  _target_: pytorch_lightning.loggers.wandb.WandbLogger
  save_dir: null # "${paths.output_dir}"
  offline: False
  id: null # pass correct id to resume experiment!
  anonymous: null # enable anonymous logging
  project: 'WavetableCVAE'
  log_model: True # upload lightning ckpts
  prefix: "" # a string to put at the beginning of metric keys
  # entity: "" # set to name of your wandb team
  group: ""
  tags: []
  job_type: ""

trainer:
  _target_: pytorch_lightning.Trainer
  max_epochs: 10001
  enable_checkpointing: True
  auto_lr_find: True
  auto_scale_batch_size: True

callbacks:
  _target_: "src.models.components.callback.MyPrintingCallback"
  print_every_n_steps: 1000

resume: null
# "/workspace/My-reserch-project/WavetableCVAE/rp6j1uld/checkpoints/epoch=1999-step=198000.ckpt"
save: False
seed: 42
debug_mode: False
comment: null